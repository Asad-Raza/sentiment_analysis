{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "max_features = 25000\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/keras/datasets/imdb.py:49: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 6s 0us/step\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,128,input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/7\n",
      "25000/25000 [==============================] - 162s 6ms/step - loss: 0.4002 - acc: 0.8150 - val_loss: 0.3240 - val_acc: 0.8629\n",
      "Epoch 2/7\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.2039 - acc: 0.9242 - val_loss: 0.3327 - val_acc: 0.8681\n",
      "Epoch 3/7\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.1278 - acc: 0.9548 - val_loss: 0.3793 - val_acc: 0.8650\n",
      "Epoch 4/7\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.0866 - acc: 0.9709 - val_loss: 0.5462 - val_acc: 0.8476\n",
      "Epoch 5/7\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0698 - acc: 0.9772 - val_loss: 0.5068 - val_acc: 0.8598\n",
      "Epoch 6/7\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.0414 - acc: 0.9876 - val_loss: 0.5886 - val_acc: 0.8569\n",
      "Epoch 7/7\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.6760 - val_acc: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f53116de6a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=7, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "loss: 0.6759736129486561\n",
      "acc: 0.84908\n",
      "saved model to disk\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy = model.evaluate(X_test,y_test)\n",
    "print('loss:',loss)\n",
    "print('acc:',accuracy)\n",
    "\n",
    "#serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "#serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model from disk\n"
     ]
    }
   ],
   "source": [
    "#predicting for new datasets\n",
    "from keras.preprocessing import text\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded Model from disk\")\n",
    "#compile and evaluate loaded model\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/keras_preprocessing/text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(nb_words=2500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textToNumeric(text):\n",
    "    #tokenizer.fit_on_texts(text)\n",
    "    numericText = tokenizer.texts_to_sequences(text)\n",
    "    paddedInput = sequence.pad_sequences(numericText,maxlen=maxlen)\n",
    "    return paddedInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-8fb1ca2628eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(text1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#tokenizer.fit_on_texts(text1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_word_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "text1 = 'Rated Have been a regular orderer from this joint generally take aways, The food quality and portions are decent. Had ordered Veg Biriyani and some Lasagna from the joint, the Biriyani was good especially the spices and a hint of mint made the same delicious ( a must try ) but Lasagna was like a small bowl of cheese tomato base and veggies utter disappointment as it was not even seasoned properly.'\n",
    "#print(text1)\n",
    "#tokenizer.fit_on_texts(text1)\n",
    "text1  = keras.preprocessing.text.keras.preprocessing.text.text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 3s 2us/step\n"
     ]
    }
   ],
   "source": [
    "#this has to be loaded for new text conversion into vectors\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 'have', 'ordered', 'two', 'medium', 'pizza', 'and', 'two', 'numbers', 'of', 'chicken', 'wings', 'i', 'have', 'made', 'online', 'payment', 'through', 'my', 'debit', 'card', 'my', 'order', 'no', 'is', '232', 'dated', '27', '06', '2018', 'amounting', 'to', 'rs', '1432', 'payment', 'has', 'been', 'done', 'and', 'then', 'i', 'received', 'a', 'call', 'from', 'protected', 'telling', 'me', 'that', 'chicken', 'wings', 'are', 'out', 'of', 'stock', 'the', 'payment', 'for', 'the', 'same', 'will', 'be', 'send', 'back', 'in', '7', 'to', '8', 'days', 'the', 'main', 'issue', 'is', 'that', 'f', 'it', 'was', 'not', 'in', 'stock', 'then', 'why', 'it', 'was', 'not', 'displayed', 'now', 'after', 'payment', 'it', 'is', 'being', 'said', 'that', 'it', 'is', 'out', 'of', 'stock', 'will', 'you', 'let', 'me', 'know', 'how', 'would', 'you', 'compensate', 'for', 'my', 'grievance', 'all', 'my', 'friends', 'and', 'me', 'are', 'totally', 'frustrated']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text2 = \"Hi I have ordered two medium pizza and two numbers of chicken wings. I have made online payment through my debit card. My order No. is 232 dated 27.06.2018 amounting to Rs. 1432. Payment has been done and then I received a call from protected telling me that chicken wings are out of stock. The payment for the same will be send back in 7 to 8 days.The main issue is that f it was not in stock then why it was not displayed. Now after payment it is being said that it is out of stock. Will you let me know how would you compensate for my grievance all My friends and Me are totally frustrated\"\n",
    "# tokenize the document\n",
    "tokenizedText= text_to_word_sequence(text2)\n",
    "print(tokenizedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6593,    10,    25,  5172,   104,  3446,  7754,     2,   104,\n",
       "        1393,     4,  5142,  5831,    10,    25,    90,  4689, 11334,\n",
       "         140,    58,     0,  3152,    58,   658,    54,     6,     0,\n",
       "        1964,  7508, 19839,     0,     0,     5,     0,     0, 11334,\n",
       "          44,    74,   221,     2,    92,    10,  1987,     3,   680,\n",
       "          36, 14844,   976,    69,    12,  5142,  5831,    23,    43,\n",
       "           4,  2050,     1, 11334,    15,     1,   169,    77,    27,\n",
       "        2219,   142,     8,   690,     5,   706,   501,     1,   290,\n",
       "        1831,     6,    12,  1206,     9,    13,    21,     8,  2050,\n",
       "          92,   135,     9,    13,    21,  4339,   147,   100, 11334,\n",
       "           9,     6,   109,   298,    12,     9,     6,    43,     4,\n",
       "        2050,    77,    22,   384,    69,   121,    86,    59,    22,\n",
       "        7965,    15,    58,     0,    29,    58,   366,     2,    69,\n",
       "          23,   481,  3568])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericText = np.array([word_index[word] if (word in word_index) and (word_index[word]<25000) else 0 for word in tokenizedText])\n",
    "numericText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numericText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numericText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,  6593,\n",
       "           10,    25,  5172,   104,  3446,  7754,     2,   104,  1393,\n",
       "            4,  5142,  5831,    10,    25,    90,  4689, 11334,   140,\n",
       "           58,     0,  3152,    58,   658,    54,     6,     0,  1964,\n",
       "         7508, 19839,     0,     0,     5,     0,     0, 11334,    44,\n",
       "           74,   221,     2,    92,    10,  1987,     3,   680,    36,\n",
       "        14844,   976,    69,    12,  5142,  5831,    23,    43,     4,\n",
       "         2050,     1, 11334,    15,     1,   169,    77,    27,  2219,\n",
       "          142,     8,   690,     5,   706,   501,     1,   290,  1831,\n",
       "            6,    12,  1206,     9,    13,    21,     8,  2050,    92,\n",
       "          135,     9,    13,    21,  4339,   147,   100, 11334,     9,\n",
       "            6,   109,   298,    12,     9,     6,    43,     4,  2050,\n",
       "           77,    22,   384,    69,   121,    86,    59,    22,  7965,\n",
       "           15,    58,     0,    29,    58,   366,     2,    69,    23,\n",
       "          481,  3568]], dtype=int32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_inp = sequence.pad_sequences([numericText],maxlen=maxlen)\n",
    "numeric_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9787288]]\n"
     ]
    }
   ],
   "source": [
    "out = loaded_model.predict(numeric_inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    tokenizedText= text_to_word_sequence(text)\n",
    "    numericText = np.array([word_index[word] if (word in word_index) and (word_index[word]<25000) else 0 for word in tokenizedText])\n",
    "    numeric_inp = sequence.pad_sequences([numericText],maxlen=maxlen)\n",
    "    out = loaded_model.predict(numeric_inp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"Hi I have ordered two medium pizza and two numbers of chicken wings. I have made online payment through my debit card. My order No. is 232 dated 27.06.2018 amounting to Rs. 1432. Payment has been done and then I received a call from protected telling me that chicken wings are out of stock. The payment for the same will be send back in 7 to 8 days.The main issue is that f it was not in stock then why it was not displayed. Now after payment it is being said that it is out of stock. Will you let me know how would you compensate for my grievance all My friends and Me are totally frustrated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9787288]]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
